{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install pythainlp"
      ],
      "metadata": {
        "id": "m-yqfdSwC-yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1K0VgdwYagNidu5k_y5pnSrGrGIOBPsyS' -O Fastfood_Opinion.csv"
      ],
      "metadata": {
        "id": "019Mxh_vWudq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "FxCWaC0AGYI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load and preprocess the data\n",
        "data = pd.read_csv('Fastfood_Opinion.csv')  # Replace 'your_dataset.csv' with the path to your dataset\n",
        "X = data['message'].astype(str)  # Text data in the 'message' column\n",
        "y = data['class']  # Labels in the 'class' column\n",
        "\n",
        "\n",
        "# Step 2: Create bag-of-words representation\n",
        "# Preprocess the data Before training the model, you need to preprocess the Thai text data to convert it into tokenized\n",
        "# and numerical features. We'll use scikit-learn's CountVectorizer to transform the text data into bag-of-word features (bow).\n",
        "\n",
        "# Tokenize Thai text\n",
        "X_tokenized = X.apply(word_tokenize, keep_whitespace=False)\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer=lambda x: x)  # Use the list of tokens as the analyzer\n",
        "X_bow = vectorizer.fit_transform(X_tokenized)\n",
        "print(X_bow.shape) # (documents, vocab)\n",
        "\n",
        "vocab = np.array(vectorizer.get_feature_names_out())\n",
        "print(vocab.shape)\n",
        "print(vocab[250:270])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3CKw6JNE3nB",
        "outputId": "04eddda0-7574-4776-9b33-beae54f4575b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(642, 3549)\n",
            "(3549,)\n",
            "['Motorway' 'Mushroom' 'Nugget' 'Operation' 'Order' 'PM' 'POS' 'Paradise'\n",
            " 'Park' 'Piroj' 'Pizza' 'Place' 'Plaza' 'Pork' 'QR' 'Qhappy' 'Qr' 'Rayong'\n",
            " 'Refill' 'Riverside']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_tokenized[2])\n",
        "print(X_bow[2])\n",
        "print(vocab[1021])\n",
        "print(vocab[1069])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvU2KsBTKOrW",
        "outputId": "8f4d0042-79bf-44a0-cee9-4abceb8c4eac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Burger', 'King', 'สาขา', 'The', 'Bright', 'พระราม', '2', 'บริการ', 'ห่วย', 'มาก', 'ถึง', 'มาก', 'ที่สุด', 'วันนี้', 'ไป', 'ทาน', 'กับ', 'ลูก', '3', 'คน', 'พนักงาน', 'รับ', 'ออเดอร์', 'มัวแต่', 'คุย', 'เล่น', 'กัน', 'ยืน', 'กัน', 'หลาย', 'คน', 'แต่', 'เปิด', 'เคา', 'เตอร์', 'เดียว', 'กว่า', 'จะ', 'สั่ง', 'ได้', 'นาน', 'มาก', 'พอ', 'สั่ง', 'แล้ว', 'เฟรนฟ', 'ราย', 'หมด', 'พอดี', 'พนักงาน', 'บอ', 'กว่า', 'เดี๋ยว', 'เอา', 'ไป', 'ให้', 'ที่', 'โต๊ะ', 'ก็', 'ไม่เป็นไร', 'ก็', 'ไป', 'นั่ง', 'รอ', 'ก็', 'นั่ง', 'ป้อน', 'แฮมเบอร์เกอร์', 'ลูก', 'ๆ', 'ไป', 'ซัก', 'พักใหญ่', 'เฟรนฟ', 'ราย', 'ก็', 'ยัง', 'ไม่', 'มา', 'แต่', 'เห็น', 'โต๊ะ', 'อื่น', 'ที่มา', 'หลัง', 'เรา', 'ได้', 'แล้ว', 'ก็', 'ลุก', 'ไป', 'ตาม', 'กับ', 'ผู้จัดการ', '(', 'แต่งตัว', 'ไม่', 'เหมือน', 'คนอื่น', ')', 'ตาม', 'ครั้ง', 'ที่', '1', 'ก็', 'ได้รับ', 'คำตอบ', 'เหมือนเดิม', 'ไป', 'นั่ง', 'รอ', 'ที่', 'โต๊ะ', 'เดี๋ยว', 'ไป', 'เส', 'ริ', 'ฟ', 'ให้', 'จน', 'ป้อน', 'เด็ก', 'จน', 'หมด', 'ใช้เวลา', 'ประมาณ', '30', 'นาที', 'ก็', 'เดิน', 'ไป', 'ตาม', 'รอบ', 'ที่', '2', 'ทาง', 'ผู้จัดการ', 'ยื่น', 'มา', 'ให้', '1', 'ห่อ', 'ก่อน', '(', 'สั่ง', '2', 'ชุด', ')', 'บอ', 'กว่า', 'อีก', 'ห่อ', 'เดี๋ยว', 'ค่อย', 'เอา', 'ไป', 'เส', 'ริ', 'ฟ', 'ที่', 'โต๊ะ', '(', 'อีก', 'ละ', ')', 'จน', 'กลับมา', 'ที่', 'โต๊ะ', 'กิน', 'จน', 'หมด', 'อิ่ม', 'แล้ว', 'เฟรนฟ', 'ราย', 'อีก', 'ห่อ', 'ก็', 'ยัง', 'ไม่', 'มา', 'เส', 'ริ', 'ฟ', 'จน', 'ต้อง', 'ไป', 'ตาม', 'ครั้ง', 'ที่', '3', 'กับ', 'พนักงาน', 'เคา', 'เตอร์', 'พนักงาน', 'เข้าไป', 'คุย', 'กับ', 'ผู้จัดการ', 'เบา', 'ๆ', '(', 'แต่', 'แอบ', 'ได้ยิน', ')', '\"', 'พี่', 'ๆ', 'หนู', 'บอก', 'แล้ว', 'ว่า', 'จะ', 'ไป', 'เส', 'ริ', 'ฟ', 'พี่', 'บอ', 'กว่า', 'ไม่ต้อง', 'ลูกค้า', 'กลับ', 'ไป', 'แล้ว', 'เนี่ย', 'เขา', 'มา', 'ตาม', '\"', 'ได้ยิน', 'โมโห', 'มาก', 'เนี่ย', 'ถ้า', 'ไม่', 'ตาม', 'ก็', 'คง', 'ไม่', 'มา', 'เส', 'ริ', 'ฟ', 'ใช่', 'มั้ย', 'ขอ', 'บอ', 'กว่า', 'ใน', 'บรรดา', 'เบอร์เกอร์', 'ชอบ', 'กิน', 'Burger', 'King', 'มาก', 'ที่', 'สุดแต่', 'เจอ', 'พนักงาน', 'ยัน', 'ผู้จัดการ', 'แบบนี้', 'ขอ', 'บาย', 'ค่ะ']\n",
            "  (0, 3483)\t12\n",
            "  (0, 1347)\t8\n",
            "  (0, 2357)\t1\n",
            "  (0, 3424)\t1\n",
            "  (0, 2154)\t1\n",
            "  (0, 109)\t3\n",
            "  (0, 485)\t1\n",
            "  (0, 1242)\t1\n",
            "  (0, 857)\t2\n",
            "  (0, 1315)\t1\n",
            "  (0, 2913)\t1\n",
            "  (0, 1881)\t5\n",
            "  (0, 1530)\t1\n",
            "  (0, 678)\t2\n",
            "  (0, 572)\t9\n",
            "  (0, 3209)\t3\n",
            "  (0, 1882)\t5\n",
            "  (0, 697)\t2\n",
            "  (0, 846)\t5\n",
            "  (0, 3465)\t2\n",
            "  (0, 555)\t2\n",
            "  (0, 3493)\t5\n",
            "  (0, 3146)\t2\n",
            "  (0, 2247)\t1\n",
            "  (0, 675)\t1\n",
            "  :\t:\n",
            "  (0, 487)\t1\n",
            "  (0, 2636)\t1\n",
            "  (0, 1225)\t1\n",
            "  (0, 2728)\t1\n",
            "  (0, 2889)\t1\n",
            "  (0, 3315)\t1\n",
            "  (0, 3474)\t2\n",
            "  (0, 12)\t2\n",
            "  (0, 1786)\t2\n",
            "  (0, 2453)\t1\n",
            "  (0, 1538)\t1\n",
            "  (0, 3496)\t1\n",
            "  (0, 2182)\t1\n",
            "  (0, 2871)\t2\n",
            "  (0, 3381)\t1\n",
            "  (0, 3412)\t1\n",
            "  (0, 1878)\t1\n",
            "  (0, 1528)\t1\n",
            "  (0, 2886)\t1\n",
            "  (0, 918)\t1\n",
            "  (0, 2396)\t1\n",
            "  (0, 2758)\t1\n",
            "  (0, 1966)\t1\n",
            "  (0, 1562)\t1\n",
            "  (0, 814)\t1\n",
            "ญ\n",
            "ดๆ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train the binomial logistic regression\n",
        "logreg_classifier = LogisticRegression()\n",
        "logreg_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred = logreg_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZyiESEvJ6Fn",
        "outputId": "4d008608-c51f-4e91-a6b4-1bf4fcaf1850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8992248062015504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Print classification report (precision, recall, F1-score, support)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV0I0HVf8sUu",
        "outputId": "9df0a829-86e7-49e7-e3ae-3661e654952d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.86      0.91        74\n",
            "           1       0.84      0.95      0.89        55\n",
            "\n",
            "    accuracy                           0.90       129\n",
            "   macro avg       0.90      0.91      0.90       129\n",
            "weighted avg       0.91      0.90      0.90       129\n",
            "\n",
            "Confusion Matrix:\n",
            "[[64 10]\n",
            " [ 3 52]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict New Data: Example 1\n",
        "## *** ให้นิสิตแก้ไข code เพื่อให้ส่งข้อมูลตัดคำไปทำ vectorizer ได้ถูกต้อง และจะทำให้ classify ได้ถูกต้อง\n",
        "\n",
        "#new_text = \"ของไม่ตรงปก ส่งช้า แพ็คไม่ดี\"\n",
        "new_text = \"มากินบ่อยมาก แต่ให้ 1 ดาวนะ\"\n",
        "new_text_tokenized = word_tokenize(new_text , keep_whitespace=False)\n",
        "new_text_bow = vectorizer.transform([new_text_tokenized])\n",
        "print(new_text_bow)\n",
        "\n",
        "y_pred = logreg_classifier.predict(new_text_bow)\n",
        "print('predict : ',y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIzm3Uyu80sf",
        "outputId": "7fe272f0-fc4d-43b4-adbd-23189af4317a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 51)\t1\n",
            "  (0, 555)\t1\n",
            "  (0, 1038)\t1\n",
            "  (0, 1426)\t1\n",
            "  (0, 1585)\t1\n",
            "  (0, 1881)\t1\n",
            "  (0, 1882)\t1\n",
            "  (0, 3209)\t1\n",
            "  (0, 3441)\t1\n",
            "predict :  [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict New Data: Example 2\n",
        "\n",
        "new_text = \"อยากให้ 5 ดาวนะแต่ยังไม่ใช่\"\n",
        "new_text_tokenized = word_tokenize(new_text , keep_whitespace=False)\n",
        "new_text_bow = vectorizer.transform([new_text_tokenized])\n",
        "print(new_text_bow)\n",
        "\n",
        "y_pred = logreg_classifier.predict(new_text_bow)\n",
        "print('predict : ',y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1wpCC7d9VHc",
        "outputId": "6dd75b37-e292-49c7-f6b9-dd0b8d083ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 150)\t1\n",
            "  (0, 1038)\t1\n",
            "  (0, 1426)\t1\n",
            "  (0, 1957)\t1\n",
            "  (0, 2575)\t1\n",
            "  (0, 3209)\t1\n",
            "  (0, 3412)\t1\n",
            "  (0, 3441)\t1\n",
            "  (0, 3493)\t1\n",
            "predict :  [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bays"
      ],
      "metadata": {
        "id": "DYe8IIPJB_dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "\n",
        "# Step 1: Load and preprocess the data\n",
        "data = pd.read_csv('Fastfood_Opinion.csv')  # Replace 'your_dataset.csv' with the path to your dataset\n",
        "X = data['message'].astype(str)  # Text data in the 'message' column\n",
        "y = data['class']  # Labels in the 'class' column\n",
        "\n",
        "# Tokenize Thai text\n",
        "X_tokenized = X.apply(word_tokenize, keep_whitespace=False)\n",
        "\n",
        "# Step 2: Create bag-of-words representation\n",
        "vectorizer2 = CountVectorizer(analyzer=lambda x: x)  # Use the list of tokens as the analyzer\n",
        "X_bow = vectorizer2.fit_transform(X_tokenized)\n",
        "print(X_bow.shape) # (documents, vocab)\n",
        "\n",
        "# Step 3: Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train the binomial Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFsl51sbB_pu",
        "outputId": "8f3b2ad7-f34d-46b2-94ef-dda94bfbb54e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(642, 3549)\n",
            "Accuracy: 0.8914728682170543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MaGsAstV-go",
        "outputId": "9cf330ec-0d2a-496c-d715-95600d304be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.96      0.91        74\n",
            "           1       0.94      0.80      0.86        55\n",
            "\n",
            "    accuracy                           0.89       129\n",
            "   macro avg       0.90      0.88      0.89       129\n",
            "weighted avg       0.90      0.89      0.89       129\n",
            "\n",
            "Confusion Matrix:\n",
            "[[71  3]\n",
            " [11 44]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"มากินบ่อยมาก แต่ให้ 1 ดาวนะ\"\n",
        "new_text_tokenized = word_tokenize(new_text , keep_whitespace=False)\n",
        "new_text_bow = vectorizer2.transform([new_text_tokenized])\n",
        "print(new_text_bow)\n",
        "\n",
        "y_pred = nb_classifier.predict(new_text_bow)\n",
        "print('predict : ',y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyq0tsWmYlEn",
        "outputId": "0f583164-0cc1-48bc-dfb8-dffe6e363f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 51)\t1\n",
            "  (0, 555)\t1\n",
            "  (0, 1038)\t1\n",
            "  (0, 1426)\t1\n",
            "  (0, 1585)\t1\n",
            "  (0, 1881)\t1\n",
            "  (0, 1882)\t1\n",
            "  (0, 3209)\t1\n",
            "  (0, 3441)\t1\n",
            "predict :  [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"อยากให้ 5 ดาวนะแต่ยังไม่ใช่\"\n",
        "new_text_tokenized = word_tokenize(new_text , keep_whitespace=False)\n",
        "new_text_bow = vectorizer2.transform([new_text_tokenized])\n",
        "print(new_text_bow)\n",
        "\n",
        "y_pred = nb_classifier.predict(new_text_bow)\n",
        "print('predict : ',y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ECTprmJYms6",
        "outputId": "13c3b799-258f-459f-e5d4-35f7a9a21428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 150)\t1\n",
            "  (0, 1038)\t1\n",
            "  (0, 1426)\t1\n",
            "  (0, 1957)\t1\n",
            "  (0, 2575)\t1\n",
            "  (0, 3209)\t1\n",
            "  (0, 3412)\t1\n",
            "  (0, 3441)\t1\n",
            "  (0, 3493)\t1\n",
            "predict :  [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DecisionTree"
      ],
      "metadata": {
        "id": "i7XBbSHzUZ92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "\n",
        "# Step 1: Load and preprocess the data\n",
        "data = pd.read_csv('Fastfood_Opinion.csv')  # Replace 'your_dataset.csv' with the path to your dataset\n",
        "X = data['message'].astype(str)  # Text data in the 'message' column\n",
        "y = data['class']  # Labels in the 'class' column\n",
        "\n",
        "# Tokenize Thai text\n",
        "X_tokenized = X.apply(word_tokenize, keep_whitespace=False)\n",
        "\n",
        "# Step 2: Create bag-of-words representation\n",
        "vectorizer3 = CountVectorizer(analyzer=lambda x: x)  # Use the list of tokens as the analyzer\n",
        "X_bow = vectorizer3.fit_transform(X_tokenized)\n",
        "print(X_bow.shape) # (documents, vocab)\n",
        "\n",
        "data = pd.read_csv('Fastfood_Opinion.csv')\n",
        "X = data['message'].astype(str)\n",
        "y = data['class']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
        "clf = DecisionTreeClassifier()\n",
        "# Train Decision Tree Classifer\n",
        "clf = clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11zspQt8T-AG",
        "outputId": "36a350a0-d78c-40f8-a1eb-ccba248c5d0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(642, 3549)\n",
            "Accuracy: 0.875968992248062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc5zeJRnWFpx",
        "outputId": "0417bf2b-e7fe-4ec7-e940-566950187d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.89      0.89        74\n",
            "           1       0.85      0.85      0.85        55\n",
            "\n",
            "    accuracy                           0.88       129\n",
            "   macro avg       0.87      0.87      0.87       129\n",
            "weighted avg       0.88      0.88      0.88       129\n",
            "\n",
            "Confusion Matrix:\n",
            "[[66  8]\n",
            " [ 8 47]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"มากินบ่อยมาก แต่ให้ 1 ดาวนะ\"\n",
        "new_text_tokenized = word_tokenize(new_text , keep_whitespace=False)\n",
        "new_text_bow = vectorizer3.transform([new_text_tokenized])\n",
        "print(new_text_bow)\n",
        "\n",
        "y_pred = clf.predict(new_text_bow)\n",
        "print('predict : ',y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsN-M3VwYx3A",
        "outputId": "c2065baf-b211-4654-aa58-7e20fb80b0f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 51)\t1\n",
            "  (0, 555)\t1\n",
            "  (0, 1038)\t1\n",
            "  (0, 1426)\t1\n",
            "  (0, 1585)\t1\n",
            "  (0, 1881)\t1\n",
            "  (0, 1882)\t1\n",
            "  (0, 3209)\t1\n",
            "  (0, 3441)\t1\n",
            "predict :  [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"อยากให้ 5 ดาวนะแต่ยังไม่ใช่\"\n",
        "new_text_tokenized = word_tokenize(new_text , keep_whitespace=False)\n",
        "new_text_bow = vectorizer3.transform([new_text_tokenized])\n",
        "print(new_text_bow)\n",
        "\n",
        "y_pred = clf.predict(new_text_bow)\n",
        "print('predict : ',y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH0t_TYjY8VP",
        "outputId": "54165e78-eb73-4e06-a54a-355b92141c19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 150)\t1\n",
            "  (0, 1038)\t1\n",
            "  (0, 1426)\t1\n",
            "  (0, 1957)\t1\n",
            "  (0, 2575)\t1\n",
            "  (0, 3209)\t1\n",
            "  (0, 3412)\t1\n",
            "  (0, 3441)\t1\n",
            "  (0, 3493)\t1\n",
            "predict :  [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "iq1EUJ4xVyfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "a0IDpRQcWN-j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9p6l7CNWEHq"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('Fastfood_Opinion.csv')\n",
        "X = data['message'].astype(str)\n",
        "y = data['class']"
      ],
      "metadata": {
        "id": "Y9b2blo1WZ-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tokenized = X.apply(word_tokenize, keep_whitespace=False)\n",
        "vectorizer = TfidfVectorizer(analyzer=lambda x: x)\n",
        "X_bow = vectorizer.fit_transform(X_tokenized)\n",
        "print(X_bow.shape)\n",
        "vocab = np.array(vectorizer.get_feature_names_out())\n",
        "print(vocab.shape)\n",
        "print(vocab[3500:3510])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3kYAzV6XR9l",
        "outputId": "c8971855-6517-4ea5-e737-5d276ebf1b5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(642, 3549)\n",
            "(3549,)\n",
            "['ไม่เท่าไหร่' 'ไม่เป็นไร' 'ไม้' 'ไร' 'ไร้' 'ไลน์' 'ไล่' 'ไล่ออก' 'ไว'\n",
            " 'ไว้']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_tokenized[0])\n",
        "print(X_bow[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzq0eJjhZDLI",
        "outputId": "1278ce64-e8ac-4a4c-93ee-c459e2b5cf92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['11.30', 'น.', 'ไป', 'ที่', 'สาขา', 'ใน', 'ปั๊ม', 'คาลเท็กซ์', 'ประ', 'ชานุ', 'กู', 'ล', 'ซื้อ', 'พาย', 'ไก่', 'และ', 'พาย', 'เผือก', 'อย่าง', 'ละ', '2', 'ชิ้น', 'กลับ', 'ถึง', 'บ้าน', 'จะ', 'ทาน', 'พาย', 'ไก่', 'เปิด', 'มา', 'กลายเป็น', 'พาย', 'เชอรี่', 'ทั้งสอง', 'ชิ้น', 'เลย', 'แย่จัง']\n",
            "  (0, 3280)\t0.19967979047201942\n",
            "  (0, 3034)\t0.057450836701905275\n",
            "  (0, 1300)\t0.17924460163754558\n",
            "  (0, 2773)\t0.19967979047201942\n",
            "  (0, 495)\t0.1672907824742227\n",
            "  (0, 1881)\t0.05473795894025931\n",
            "  (0, 2913)\t0.1099219545165859\n",
            "  (0, 1315)\t0.0738016565088901\n",
            "  (0, 857)\t0.0646577478188027\n",
            "  (0, 1590)\t0.11210678689789609\n",
            "  (0, 1242)\t0.09615330121272804\n",
            "  (0, 485)\t0.11287258957964276\n",
            "  (0, 948)\t0.23775008436136766\n",
            "  (0, 109)\t0.09659664408741996\n",
            "  (0, 2154)\t0.11529711226976311\n",
            "  (0, 2584)\t0.1059852159708011\n",
            "  (0, 2942)\t0.1672907824742227\n",
            "  (0, 3291)\t0.08235182841801443\n",
            "  (0, 3453)\t0.2458959106262061\n",
            "  (0, 1772)\t0.6089230055717042\n",
            "  (0, 1006)\t0.11615171545717369\n",
            "  (0, 2135)\t0.14231096251438263\n",
            "  (0, 570)\t0.1553369633108998\n",
            "  (0, 935)\t0.17924460163754558\n",
            "  (0, 1602)\t0.1627461513488565\n",
            "  (0, 772)\t0.18772597130869656\n",
            "  (0, 1649)\t0.15223075139292605\n",
            "  (0, 3424)\t0.08235182841801443\n",
            "  (0, 2357)\t0.05127751097302055\n",
            "  (0, 1347)\t0.05583815173526733\n",
            "  (0, 3483)\t0.05441575086328181\n",
            "  (0, 1404)\t0.13658690429164758\n",
            "  (0, 64)\t0.17924460163754558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
        "\n",
        "logreg_classifier = LogisticRegression()\n",
        "logreg_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = logreg_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3ul0buUZGK8",
        "outputId": "f00bf946-d72f-4ab9-b209-088efe0648a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9069767441860465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggAykIpqZ7f7",
        "outputId": "17b17665-e73a-495d-c330-de14d18eb2b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.95      0.92        74\n",
            "           1       0.92      0.85      0.89        55\n",
            "\n",
            "    accuracy                           0.91       129\n",
            "   macro avg       0.91      0.90      0.90       129\n",
            "weighted avg       0.91      0.91      0.91       129\n",
            "\n",
            "Confusion Matrix:\n",
            "[[70  4]\n",
            " [ 8 47]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"มากินบ่อยมาก แต่ให้ 1 ดาวนะ\"\n",
        "new_text_tokenized = word_tokenize(new_text , keep_whitespace=False)\n",
        "new_text_bow = vectorizer.transform([new_text_tokenized])\n",
        "print(new_text_bow)\n",
        "\n",
        "y_pred = logreg_classifier.predict(new_text_bow)\n",
        "print('predict : ',y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UableWagaHOY",
        "outputId": "1b689bde-fcf8-41af-9d5c-f9bfd681453d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 3441)\t0.2443973805947284\n",
            "  (0, 3209)\t0.239151800824748\n",
            "  (0, 1882)\t0.2073723113408591\n",
            "  (0, 1881)\t0.19961658506184696\n",
            "  (0, 1585)\t0.4406797805644825\n",
            "  (0, 1426)\t0.32380755278152207\n",
            "  (0, 1038)\t0.5449029992881355\n",
            "  (0, 555)\t0.26989192991037647\n",
            "  (0, 51)\t0.36251254775171565\n",
            "predict :  [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"อยากให้ 5 ดาวนะแต่ยังไม่ใช่\"\n",
        "new_text_tokenized = word_tokenize(new_text , keep_whitespace=False)\n",
        "new_text_bow = vectorizer.transform([new_text_tokenized])\n",
        "print(new_text_bow)\n",
        "\n",
        "y_pred = logreg_classifier.predict(new_text_bow)\n",
        "print('predict : ',y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNUTgm0Le2cM",
        "outputId": "152cf84e-dc3b-4e8a-ba69-2ea4cddae0f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 3493)\t0.1648582280481312\n",
            "  (0, 3441)\t0.22766686294079466\n",
            "  (0, 3412)\t0.3783049103099124\n",
            "  (0, 3209)\t0.22278037566490402\n",
            "  (0, 2575)\t0.2993384439208498\n",
            "  (0, 1957)\t0.30641101376191476\n",
            "  (0, 1426)\t0.30164091594971476\n",
            "  (0, 1038)\t0.507601006823703\n",
            "  (0, 150)\t0.44283927733497297\n",
            "predict :  [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bays"
      ],
      "metadata": {
        "id": "LSy1Gih-WPFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "data = pd.read_csv('Fastfood_Opinion.csv')\n",
        "X = data['message'].astype(str)\n",
        "y = data['class']\n",
        "\n",
        "X_tokenized = X.apply(word_tokenize, keep_whitespace=False)\n",
        "\n",
        "vectorizer2 = TfidfVectorizer(analyzer=lambda x: x)\n",
        "X_bow = vectorizer2.fit_transform(X_tokenized)\n",
        "print(X_bow.shape)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
        "\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqIh2pgOWVLG",
        "outputId": "272b8eab-dbee-4618-ff16-cdd56310a959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(642, 3549)\n",
            "Accuracy: 0.8527131782945736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0COEWbnakkL0",
        "outputId": "65408df1-172f-4871-8a6e-71671113586f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      1.00      0.89        74\n",
            "           1       1.00      0.65      0.79        55\n",
            "\n",
            "    accuracy                           0.85       129\n",
            "   macro avg       0.90      0.83      0.84       129\n",
            "weighted avg       0.88      0.85      0.85       129\n",
            "\n",
            "Confusion Matrix:\n",
            "[[74  0]\n",
            " [19 36]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"มากินบ่อยมาก แต่ให้ 1 ดาวนะ\"\n",
        "new_text_tokenized = word_tokenize(new_text , keep_whitespace=False)\n",
        "new_text_bow = vectorizer.transform([new_text_tokenized])\n",
        "print(new_text_bow)\n",
        "\n",
        "y_pred = nb_classifier.predict(new_text_bow)\n",
        "print('predict : ',y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AILLCGiEkm09",
        "outputId": "719ede8f-c07d-4e28-c992-9ac055f093a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 3441)\t0.2443973805947284\n",
            "  (0, 3209)\t0.239151800824748\n",
            "  (0, 1882)\t0.2073723113408591\n",
            "  (0, 1881)\t0.19961658506184696\n",
            "  (0, 1585)\t0.4406797805644825\n",
            "  (0, 1426)\t0.32380755278152207\n",
            "  (0, 1038)\t0.5449029992881355\n",
            "  (0, 555)\t0.26989192991037647\n",
            "  (0, 51)\t0.36251254775171565\n",
            "predict :  [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"อยากให้ 5 ดาวนะแต่ยังไม่ใช่\"\n",
        "new_text_tokenized = word_tokenize(new_text , keep_whitespace=False)\n",
        "new_text_bow = vectorizer.transform([new_text_tokenized])\n",
        "print(new_text_bow)\n",
        "\n",
        "y_pred = nb_classifier.predict(new_text_bow)\n",
        "print('predict : ',y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjALdfqhkqLf",
        "outputId": "93669422-bec5-48f4-bbde-5d0e02890611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 3493)\t0.1648582280481312\n",
            "  (0, 3441)\t0.22766686294079466\n",
            "  (0, 3412)\t0.3783049103099124\n",
            "  (0, 3209)\t0.22278037566490402\n",
            "  (0, 2575)\t0.2993384439208498\n",
            "  (0, 1957)\t0.30641101376191476\n",
            "  (0, 1426)\t0.30164091594971476\n",
            "  (0, 1038)\t0.507601006823703\n",
            "  (0, 150)\t0.44283927733497297\n",
            "predict :  [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DecisionTree"
      ],
      "metadata": {
        "id": "_RyD4EJbWPT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "\n",
        "data = pd.read_csv('Fastfood_Opinion.csv')\n",
        "X = data['message'].astype(str)\n",
        "y = data['class']\n",
        "\n",
        "X_tokenized = X.apply(word_tokenize, keep_whitespace=False)\n",
        "\n",
        "vectorizer3 = TfidfVectorizer(analyzer=lambda x: x)\n",
        "X_bow = vectorizer3.fit_transform(X_tokenized)\n",
        "print(X_bow.shape)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf = clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DSYGx59WV_V",
        "outputId": "708103ef-0020-4590-ef04-f8418f81533c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(642, 3549)\n",
            "Accuracy: 0.7751937984496124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w30rvhVFldGM",
        "outputId": "c67417e1-07ce-430d-8114-0ff7029d68c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.77      0.80        74\n",
            "           1       0.72      0.78      0.75        55\n",
            "\n",
            "    accuracy                           0.78       129\n",
            "   macro avg       0.77      0.78      0.77       129\n",
            "weighted avg       0.78      0.78      0.78       129\n",
            "\n",
            "Confusion Matrix:\n",
            "[[57 17]\n",
            " [12 43]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"มากินบ่อยมาก แต่ให้ 1 ดาวนะ\"\n",
        "new_text_tokenized = word_tokenize(new_text , keep_whitespace=False)\n",
        "new_text_bow = vectorizer.transform([new_text_tokenized])\n",
        "print(new_text_bow)\n",
        "\n",
        "y_pred = clf.predict(new_text_bow)\n",
        "print('predict : ',y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyBK6cculr-Q",
        "outputId": "710b2d0b-8d04-4d06-843f-08ba5a369224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 3441)\t0.2443973805947284\n",
            "  (0, 3209)\t0.239151800824748\n",
            "  (0, 1882)\t0.2073723113408591\n",
            "  (0, 1881)\t0.19961658506184696\n",
            "  (0, 1585)\t0.4406797805644825\n",
            "  (0, 1426)\t0.32380755278152207\n",
            "  (0, 1038)\t0.5449029992881355\n",
            "  (0, 555)\t0.26989192991037647\n",
            "  (0, 51)\t0.36251254775171565\n",
            "predict :  [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"อยากให้ 5 ดาวนะแต่ยังไม่ใช่\"\n",
        "new_text_tokenized = word_tokenize(new_text , keep_whitespace=False)\n",
        "new_text_bow = vectorizer.transform([new_text_tokenized])\n",
        "print(new_text_bow)\n",
        "\n",
        "y_pred = clf.predict(new_text_bow)\n",
        "print('predict : ',y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mb1KWnslxwc",
        "outputId": "1db7e744-25e9-4ec5-fe1c-75f2bb01c28d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 3493)\t0.1648582280481312\n",
            "  (0, 3441)\t0.22766686294079466\n",
            "  (0, 3412)\t0.3783049103099124\n",
            "  (0, 3209)\t0.22278037566490402\n",
            "  (0, 2575)\t0.2993384439208498\n",
            "  (0, 1957)\t0.30641101376191476\n",
            "  (0, 1426)\t0.30164091594971476\n",
            "  (0, 1038)\t0.507601006823703\n",
            "  (0, 150)\t0.44283927733497297\n",
            "predict :  [0]\n"
          ]
        }
      ]
    }
  ]
}